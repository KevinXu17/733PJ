{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e06b67d",
   "metadata": {},
   "source": [
    "# 4 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea05f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import textstat\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import string\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import operator\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "\n",
    "from bert import tokenization\n",
    "\n",
    "import sys\n",
    "from absl import flags\n",
    "sys.argv=['preserve_unused_tokens=False']\n",
    "flags.FLAGS(sys.argv)\n",
    "tf.gfile = tf.io.gfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# tf.get_logger().setLevel('ERROR')\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f106f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/out.csv'\n",
    "out_data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99601c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "SEED = 1337\n",
    "skf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba7899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3272eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "helpfulness\n",
       "1    29479\n",
       "0     9273\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_data.helpfulness.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b486efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "total_len = len(out_data)\n",
    "val_len = round(0.15 * total_len)\n",
    "\n",
    "\n",
    "y = out_data['helpfulness']\n",
    "X = out_data[['star_rating', 'vine', 'verified_purchase', 'words_text', 'sentence_count', 'word_count', 'ARI']]\n",
    "# X = out_data[['star_rating', 'vine', 'verified_purchase', 'review', 'sentence_count', 'word_count', 'ARI',\n",
    "#        'words', 'words_text']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=val_len, stratify=y,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4580a41b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32939, 7)\n",
      "(5813, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b323d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from transformers import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertModel,BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3de4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f29a7656",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47866310",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = out_data['word_count'].max()\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embeddings = np.load('glove/glove.840B.300d.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ad589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_vocab(X):\n",
    "    \n",
    "    reviews = X.apply(lambda s: s.split()).values      \n",
    "    vocab = {}\n",
    "    \n",
    "    for review in reviews:\n",
    "        for word in review:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1                \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_embeddings_coverage(X, embeddings):\n",
    "    \n",
    "    vocab = build_vocab(X)    \n",
    "    \n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov, vocab_coverage, text_coverage\n",
    "\n",
    "\n",
    "train_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(X_train['words_text'], fasttext_embeddings)\n",
    "test_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(X_test['words_text'], fasttext_embeddings)\n",
    "print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_fasttext_vocab_coverage, train_fasttext_text_coverage))\n",
    "print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_fasttext_vocab_coverage, test_fasttext_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationReport(Callback):\n",
    "    \n",
    "    def __init__(self, train_data=(), validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        \n",
    "        self.X_train, self.y_train = train_data\n",
    "        self.train_precision_scores = []\n",
    "        self.train_recall_scores = []\n",
    "        self.train_f1_scores = []\n",
    "        \n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.val_precision_scores = []\n",
    "        self.val_recall_scores = []\n",
    "        self.val_f1_scores = [] \n",
    "               \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_predictions = np.round(self.model.predict(self.X_train, verbose=0))        \n",
    "        train_precision = precision_score(self.y_train, train_predictions, average='macro')\n",
    "        train_recall = recall_score(self.y_train, train_predictions, average='macro')\n",
    "        train_f1 = f1_score(self.y_train, train_predictions, average='macro')\n",
    "        self.train_precision_scores.append(train_precision)        \n",
    "        self.train_recall_scores.append(train_recall)\n",
    "        self.train_f1_scores.append(train_f1)\n",
    "        \n",
    "        val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n",
    "        val_precision = precision_score(self.y_val, val_predictions, average='macro')\n",
    "        val_recall = recall_score(self.y_val, val_predictions, average='macro')\n",
    "        val_f1 = f1_score(self.y_val, val_predictions, average='macro')\n",
    "        self.val_precision_scores.append(val_precision)        \n",
    "        self.val_recall_scores.append(val_recall)        \n",
    "        self.val_f1_scores.append(val_f1)\n",
    "        \n",
    "        print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n",
    "        print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2ecb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterDetector:\n",
    "    \n",
    "    def __init__(self, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):\n",
    "        \n",
    "        # BERT and Tokenization params\n",
    "        self.bert_layer = bert_layer\n",
    "        \n",
    "        self.max_seq_length = max_seq_length        \n",
    "        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n",
    "        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "        \n",
    "        # Learning control params\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.models = []\n",
    "        self.scores = {}\n",
    "        \n",
    "        \n",
    "    def encode(self, texts):\n",
    "                \n",
    "        all_tokens = []\n",
    "        all_masks = []\n",
    "        all_segments = []\n",
    "\n",
    "        for text in texts:\n",
    "            text = self.tokenizer.tokenize(text)\n",
    "            text = text[:self.max_seq_length - 2]\n",
    "            input_sequence = ['[CLS]'] + text + ['[SEP]']\n",
    "            pad_len = self.max_seq_length - len(input_sequence)\n",
    "\n",
    "            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "            tokens += [0] * pad_len\n",
    "            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "            segment_ids = [0] * self.max_seq_length\n",
    "\n",
    "            all_tokens.append(tokens)\n",
    "            all_masks.append(pad_masks)\n",
    "            all_segments.append(segment_ids)\n",
    "\n",
    "        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        input_word_ids = Input(shape=(self.max_seq_length), dtype=tf.int32, name='input_word_ids')\n",
    "        input_mask = Input(shape=(self.max_seq_length), dtype=tf.int32, name='input_mask')\n",
    "        segment_ids = Input(shape=(self.max_seq_length), dtype=tf.int32, name='segment_ids')    \n",
    "        \n",
    "        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n",
    "        clf_output = sequence_output[:, 0, :]\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\n",
    "        \n",
    "        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "        optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train(self, X):\n",
    "        \n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(X['words_text'], X['verified_purchase'])):\n",
    "            \n",
    "            print('\\nFold {}\\n'.format(fold))\n",
    "            #print(trn_idx)\n",
    "            X_trn_encoded = self.encode(X.loc[trn_idx, 'words_text'].str.lower())\n",
    "            y_trn = X.loc[trn_idx, 'helpfulness']\n",
    "            X_val_encoded = self.encode(X.loc[val_idx, 'words_text'].str.lower())\n",
    "            y_val = X.loc[val_idx, 'helpfulness']\n",
    "        \n",
    "            # Callbacks\n",
    "            metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n",
    "            \n",
    "            # Model\n",
    "            model = self.build_model()        \n",
    "            model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks=[metrics], epochs=self.epochs, batch_size=self.batch_size)\n",
    "            \n",
    "            self.models.append(model)\n",
    "            self.scores[fold] = {\n",
    "                'train': {\n",
    "                    'precision': metrics.train_precision_scores,\n",
    "                    'recall': metrics.train_recall_scores,\n",
    "                    'f1': metrics.train_f1_scores                    \n",
    "                },\n",
    "                'validation': {\n",
    "                    'precision': metrics.val_precision_scores,\n",
    "                    'recall': metrics.val_recall_scores,\n",
    "                    'f1': metrics.val_f1_scores                    \n",
    "                }\n",
    "            }\n",
    "                    \n",
    "                \n",
    "    def plot_learning_curve(self):\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n",
    "    \n",
    "        for i in range(K):\n",
    "            \n",
    "            # Classification Report curve\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n",
    "\n",
    "            axes[i][0].legend() \n",
    "            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n",
    "\n",
    "            # Loss curve\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n",
    "            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n",
    "\n",
    "            axes[i][1].legend() \n",
    "            axes[i][1].set_title('Fold {} Train / Validation Loss'.format(i), fontsize=14)\n",
    "\n",
    "            for j in range(2):\n",
    "                axes[i][j].set_xlabel('Epoch', size=12)\n",
    "                axes[i][j].tick_params(axis='x', labelsize=12)\n",
    "                axes[i][j].tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_test_encoded = self.encode(X['words_text'].str.lower())\n",
    "        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n",
    "\n",
    "        for model in self.models:\n",
    "            y_pred += model.predict(X_test_encoded) / len(self.models)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e54beb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_y = pd.concat([X_train, y_train], axis=1, join='inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1576a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "clf = DisasterDetector(bert_layer, max_seq_length=128, lr=0.0001, epochs=2, batch_size=32)\n",
    "clf.train(X_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd08663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3b515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2557554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52647591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70396d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b0601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0eb0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230612c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7cb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cb017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c6bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab0632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb99bd5-9fbb-4d61-869d-c05d13058f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj733",
   "language": "python",
   "name": "pj733"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
